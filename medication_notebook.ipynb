{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["J1CTkwp-Bt7z","6TkkYlepBzl3"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZCGi0N6_SLR","executionInfo":{"status":"ok","timestamp":1750607959011,"user_tz":-540,"elapsed":17049,"user":{"displayName":"ê¹€ì„¸í¬","userId":"10121931475775017609"}},"outputId":"4a96661b-1043-4f4e-aad3-778642ed508b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pip install pyngrok streamlit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dH_ZdMBH0ebu","outputId":"c614dfe6-acc2-48df-b73d-a32afd7c22b0","executionInfo":{"status":"ok","timestamp":1750604724763,"user_tz":-540,"elapsed":21294,"user":{"displayName":"ê¹€ì„¸í¬","userId":"10121931475775017609"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n","Collecting streamlit\n","  Downloading streamlit-1.46.0-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n","Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n","Collecting watchdog<7,>=2.1.5 (from streamlit)\n","  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.43.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n","Downloading streamlit-1.46.0-py3-none-any.whl (10.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: watchdog, pyngrok, pydeck, streamlit\n","Successfully installed pydeck-0.9.1 pyngrok-7.2.11 streamlit-1.46.0 watchdog-6.0.0\n"]}]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","\n","ngrok_token = userdata.get(\"NGROK_API_KEY\")\n","!ngrok config add-authtoken {ngrok_token}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZdPFFvw0iLr","outputId":"d1e34836-d70c-49cf-e99d-f0fddbc465e7","executionInfo":{"status":"ok","timestamp":1750604942371,"user_tz":-540,"elapsed":2189,"user":{"displayName":"ê¹€ì„¸í¬","userId":"10121931475775017609"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TLR-mdvlWuKy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import subprocess\n","import os\n","from google.colab import userdata\n","import signal\n","import time\n","import streamlit as st\n","import torch\n","import numpy as np\n","from torch import optim\n","from PIL import Image\n","import torch.nn.functional as F\n","import cv2\n","import requests\n","import base64\n","import uuid\n","import re\n","import glob\n","from collections import defaultdict\n","from transformers import CLIPProcessor, CLIPModel\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n","from torchvision import transforms\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"],"metadata":{"id":"1BPFHW1NAEJw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### CLIP ëª¨ë¸ íŒŒì¸íŠœë‹"],"metadata":{"id":"J1CTkwp-Bt7z"}},{"cell_type":"code","source":["disposal_map = {\n","    \"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©\": \"ê²‰ í¬ì¥ì§€(ì¢…ì´ ë˜ëŠ” í”Œë¼ìŠ¤í‹± í•„ë¦„)ë¥¼ ì œê±°í•˜ê³ , í”Œë¼ìŠ¤í‹± ë¸”ë¦¬ìŠ¤í„°íŒ©ì€ ê·¸ëŒ€ë¡œ ì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ì„¸ìš”.\",\n","    \"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤\": \"ì¢…ì´ ë°•ìŠ¤ëŠ” ë¶„ë¦¬í•˜ì—¬ ì¢…ì´ ì¬í™œìš©í•¨ì— ë²„ë¦¬ê³ , ê²‰ í¬ì¥ì§€(ì¢…ì´ ë˜ëŠ” í”Œë¼ìŠ¤í‹± í•„ë¦„)ë¥¼ ì œê±°í•œ í›„ í”Œë¼ìŠ¤í‹± ë¸”ë¦¬ìŠ¤í„°íŒ©ì€ ì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ì„¸ìš”.\",\n","    \"ì•ˆì•½\": \"ìš©ê¸° ê·¸ëŒ€ë¡œ ì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ì„¸ìš”. ë‚¨ì€ ì•½ì•¡ì€ ê·¸ëŒ€ë¡œ ë‘ì…”ë„ ë©ë‹ˆë‹¤.\",\n","    \"ì—°ê³ \": \"íŠœë¸Œë¥¼ ì™„ì „íˆ ë¹„ìš°ê³  ì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ì„¸ìš”.\",\n","    \"ìœ ë¦¬ë³‘\": \"ì•½ì€ íì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ê³ , ìœ ë¦¬ë³‘ì€ ë‚´ìš©ë¬¼ì„ ì™„ì „íˆ ë¹„ìš´ í›„ ê¹¨ë—ì´ í—¹ê¶ˆ ë³‘ë¥˜ë¡œ ë¶„ë¦¬ë°°ì¶œí•˜ì„¸ìš”.\",\n","}\n","\n","labels = [\"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©\", \"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤\", \"ì•ˆì•½\", \"ì—°ê³ \", \"ìœ ë¦¬ë³‘\"]\n","\n","text_descriptions = [\n","    \"pill blister pack with plastic packaging\",\n","    \"pill blister pack in cardboard box\",\n","    \"small eye drop bottle\",\n","    \"ointment tube for skin\",\n","    \"glass bottle for medicine\"\n","]\n","\n","class SimpleMedicineDataset(Dataset):\n","    def __init__(self, data_list, processor):\n","        self.processor = processor\n","        self.data = data_list\n","        print(f\"ë°ì´í„°ì…‹ í¬ê¸°: {len(self.data)}\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            item = self.data[idx]\n","            image = Image.open(item['image_path']).convert('RGB')\n","\n","            return {\n","                'image': image,\n","                'text': item['text'],\n","                'label': item['label']\n","            }\n","\n","        except Exception as e:\n","            print(f\"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}\")\n","            return self.__getitem__((idx + 1) % len(self.data))\n","\n","def load_data(data_dir):\n","    all_data = []\n","    existing_labels = []\n","    existing_texts = []\n","    label_mapping = {}\n","\n","    new_label_idx = 0\n","    for original_idx, label_name in enumerate(labels):\n","        folder_path = os.path.join(data_dir, label_name)\n","        if os.path.exists(folder_path):\n","            img_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n","            if len(img_files) > 0:\n","                existing_labels.append(label_name)\n","                existing_texts.append(text_descriptions[original_idx])\n","                label_mapping[original_idx] = new_label_idx\n","\n","                # ë°ì´í„° ë¡œë“œ\n","                for img_file in img_files:\n","                    all_data.append({\n","                        'image_path': os.path.join(folder_path, img_file),\n","                        'label': new_label_idx,\n","                        'text': text_descriptions[original_idx]\n","                    })\n","\n","                new_label_idx += 1\n","\n","    print(f\"ì´ {len(all_data)}ê°œ ë°ì´í„° ë¡œë“œë¨\")\n","    print(f\"ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤: {len(existing_labels)}ê°œ\")\n","\n","    for i, label_name in enumerate(existing_labels):\n","        count = sum(1 for item in all_data if item['label'] == i)\n","        print(f\"{label_name}: {count}ê°œ\")\n","\n","    if len(all_data) == 0:\n","        return [], [], existing_labels, existing_texts\n","\n","    train_data, test_data = train_test_split(\n","        all_data,\n","        test_size=0.2,\n","        random_state=42,\n","        stratify=[item['label'] for item in all_data]\n","    )\n","\n","    print(f\"í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ\")\n","    print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)}ê°œ\")\n","\n","    return train_data, test_data, existing_labels, existing_texts\n","\n","def collate_fn(batch):\n","    images = [item['image'] for item in batch]\n","    texts = [item['text'] for item in batch]\n","    labels = [item['label'] for item in batch]\n","\n","    return {\n","        'images': images,\n","        'texts': texts,\n","        'labels': torch.tensor(labels, dtype=torch.long)\n","    }\n","\n","def evaluate_model(model, processor, test_data, existing_labels, existing_texts, device, batch_size=4):\n","    print(\"\\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===\")\n","\n","    test_dataset = SimpleMedicineDataset(test_data, processor)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n","\n","    model.eval()\n","    all_predictions = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            try:\n","                images = batch['images']\n","                texts = batch['texts']\n","                labels = batch['labels']\n","\n","                inputs = processor(\n","                    images=images,\n","                    text=existing_texts,\n","                    return_tensors=\"pt\",\n","                    padding=True,\n","                    truncation=True\n","                )\n","\n","                pixel_values = inputs['pixel_values'].to(device)\n","                input_ids = inputs['input_ids'].to(device)\n","                attention_mask = inputs['attention_mask'].to(device)\n","\n","                outputs = model(\n","                    pixel_values=pixel_values,\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask\n","                )\n","\n","                # ìœ ì‚¬ë„ ê³„ì‚°\n","                logits_per_image = outputs.logits_per_image\n","                predictions = torch.argmax(logits_per_image, dim=1)\n","\n","                all_predictions.extend(predictions.cpu().numpy())\n","                all_labels.extend(labels.numpy())\n","\n","            except Exception as e:\n","                print(f\"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}\")\n","                continue\n","\n","    accuracy = accuracy_score(all_labels, all_predictions)\n","\n","    print(f\"ì „ì²´ ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n","\n","    return accuracy, all_predictions, all_labels\n","\n","def train_model(data_dir, save_dir=\"/content/drive/MyDrive/trash_ai/clip_medication_test\", epochs=5, batch_size=2, learning_rate=1e-5):\n","    print(\"=== CLIP ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===\")\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    train_data, test_data, existing_labels, existing_texts = load_data(data_dir)\n","\n","    if len(train_data) == 0:\n","        print(\"âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n","        return None, None\n","\n","    print(f\"ì‹¤ì œ ì‚¬ìš©ë  í´ë˜ìŠ¤ë“¤: {existing_labels}\")\n","\n","    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","    model.to(device)\n","\n","    for param in model.parameters():\n","        param.requires_grad = True\n","\n","    train_dataset = SimpleMedicineDataset(train_data, processor)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n","    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n","\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    if len(test_data) > 0:\n","        initial_accuracy, _, _ = evaluate_model(model, processor, test_data, existing_labels, existing_texts, device)\n","\n","    model.train()\n","    best_accuracy = 0\n","\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        batch_count = 0\n","\n","        for batch_idx, batch in enumerate(train_loader):\n","            try:\n","                images = batch['images']\n","                texts = batch['texts']\n","                labels = batch['labels'].to(device)\n","\n","                inputs = processor(\n","                    images=images,\n","                    text=texts,\n","                    return_tensors=\"pt\",\n","                    padding=True,\n","                    truncation=True\n","                )\n","\n","                pixel_values = inputs['pixel_values'].to(device)\n","                input_ids = inputs['input_ids'].to(device)\n","                attention_mask = inputs['attention_mask'].to(device)\n","\n","                outputs = model(\n","                    pixel_values=pixel_values,\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    return_loss=True\n","                )\n","\n","                loss = outputs.loss\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","                batch_count += 1\n","\n","            except Exception as e:\n","                print(f\"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n","                continue\n","\n","        if batch_count > 0:\n","            avg_loss = total_loss / batch_count\n","\n","            if len(test_data) > 0:\n","                accuracy, _, _ = evaluate_model(model, processor, test_data, existing_labels, existing_texts, device)\n","\n","                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n","                if accuracy > best_accuracy:\n","                    best_accuracy = accuracy\n","                    try:\n","                        best_dir = os.path.join(save_dir, \"best\")\n","                        os.makedirs(best_dir, exist_ok=True)\n","                        model.save_pretrained(best_dir)\n","                        processor.save_pretrained(best_dir)\n","\n","                        import json\n","                        label_info = {\n","                            'labels': existing_labels,\n","                            'texts': existing_texts\n","                        }\n","                        with open(os.path.join(best_dir, 'label_info.json'), 'w', encoding='utf-8') as f:\n","                            json.dump(label_info, f, ensure_ascii=False, indent=2)\n","\n","                    except Exception as e:\n","                        print(f\"âŒ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n","                model.train()\n","\n","    # ìµœì¢… ëª¨ë¸ ì €ì¥\n","    try:\n","        final_dir = os.path.join(save_dir, \"final\")\n","        os.makedirs(final_dir, exist_ok=True)\n","        model.save_pretrained(final_dir)\n","        processor.save_pretrained(final_dir)\n","\n","        import json\n","        label_info = {\n","            'labels': existing_labels,\n","            'texts': existing_texts\n","        }\n","        with open(os.path.join(final_dir, 'label_info.json'), 'w', encoding='utf-8') as f:\n","            json.dump(label_info, f, ensure_ascii=False, indent=2)\n","\n","        print(\"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ\")\n","    except Exception as e:\n","        print(f\"âŒ ìµœì¢… ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n","\n","    if len(test_data) > 0:\n","        print(f\"\\n=== ìµœì¢… ì„±ëŠ¥ í‰ê°€ ===\")\n","        final_accuracy, predictions, true_labels = evaluate_model(model, processor, test_data, existing_labels, existing_texts, device)\n","        print(f\"ìµœê³  ë‹¬ì„± ì •í™•ë„: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n","\n","    print(\"=== í•™ìŠµ ì™„ë£Œ ===\")\n","    return model, processor\n","\n","if __name__ == \"__main__\":\n","    model, processor = train_model(\"/content/drive/MyDrive/trash_ai/Medicine_data\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Sd1pqXXijZT","executionInfo":{"status":"ok","timestamp":1748626377997,"user_tz":-540,"elapsed":280968,"user":{"displayName":"íˆí›","userId":"14574132926497786353"}},"outputId":"ae5ccc1d-4818-43b4-93db-abcffec81247"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== CLIP ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===\n","ì´ 376ê°œ ë°ì´í„° ë¡œë“œë¨\n","ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤: 5ê°œ\n","ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©: 99ê°œ\n","ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤: 162ê°œ\n","ì•ˆì•½: 37ê°œ\n","ì—°ê³ : 59ê°œ\n","ìœ ë¦¬ë³‘: 19ê°œ\n","í•™ìŠµ ë°ì´í„°: 300ê°œ\n","í…ŒìŠ¤íŠ¸ ë°ì´í„°: 76ê°œ\n","ì‹¤ì œ ì‚¬ìš©ë  í´ë˜ìŠ¤ë“¤: ['ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©', 'ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤', 'ì•ˆì•½', 'ì—°ê³ ', 'ìœ ë¦¬ë³‘']\n","ë°ì´í„°ì…‹ í¬ê¸°: 300\n","\n","=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===\n","ë°ì´í„°ì…‹ í¬ê¸°: 76\n","ì „ì²´ ì •í™•ë„: 0.5921 (59.21%)\n","\n","=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===\n","ë°ì´í„°ì…‹ í¬ê¸°: 76\n","ì „ì²´ ì •í™•ë„: 0.9211 (92.11%)\n","\n","=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===\n","ë°ì´í„°ì…‹ í¬ê¸°: 76\n","ì „ì²´ ì •í™•ë„: 0.8158 (81.58%)\n","\n","=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===\n","ë°ì´í„°ì…‹ í¬ê¸°: 76\n","ì „ì²´ ì •í™•ë„: 0.9342 (93.42%)\n","\n","=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===\n","ë°ì´í„°ì…‹ í¬ê¸°: 76\n","ì „ì²´ ì •í™•ë„: 0.9342 (93.42%)\n","\n","=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===\n","ë°ì´í„°ì…‹ í¬ê¸°: 76\n","ì „ì²´ ì •í™•ë„: 0.9211 (92.11%)\n","âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ\n","\n","=== ìµœì¢… ì„±ëŠ¥ í‰ê°€ ===\n","\n","=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===\n","ë°ì´í„°ì…‹ í¬ê¸°: 76\n","ì „ì²´ ì •í™•ë„: 0.9211 (92.11%)\n","ìµœê³  ë‹¬ì„± ì •í™•ë„: 0.9342 (93.42%)\n","=== í•™ìŠµ ì™„ë£Œ ===\n"]}]},{"cell_type":"markdown","source":["###CLIP ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"],"metadata":{"id":"6TkkYlepBzl3"}},{"cell_type":"code","source":["def detailed_evaluate_model(model, processor, test_data, existing_labels, existing_texts, device, batch_size=4):\n","    print(\"ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\")\n","    print(\"=\"*50)\n","\n","    test_dataset = SimpleMedicineDataset(test_data, processor)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n","\n","    model.eval()\n","    all_predictions = []\n","    all_labels = []\n","    class_correct = [0] * len(existing_labels)\n","    class_total = [0] * len(existing_labels)\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            try:\n","                images = batch['images']\n","                texts = batch['texts']\n","                labels = batch['labels']\n","\n","                inputs = processor(\n","                    images=images,\n","                    text=existing_texts,\n","                    return_tensors=\"pt\",\n","                    padding=True,\n","                    truncation=True\n","                )\n","\n","                pixel_values = inputs['pixel_values'].to(device)\n","                input_ids = inputs['input_ids'].to(device)\n","                attention_mask = inputs['attention_mask'].to(device)\n","\n","                outputs = model(\n","                    pixel_values=pixel_values,\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask\n","                )\n","\n","                logits_per_image = outputs.logits_per_image\n","                predictions = torch.argmax(logits_per_image, dim=1)\n","\n","                all_predictions.extend(predictions.cpu().numpy())\n","                all_labels.extend(labels.numpy())\n","\n","                for i, (pred, true) in enumerate(zip(predictions.cpu().numpy(), labels.numpy())):\n","                    class_total[true] += 1\n","                    if pred == true:\n","                        class_correct[true] += 1\n","\n","            except Exception as e:\n","                print(f\"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}\")\n","                continue\n","\n","    overall_accuracy = accuracy_score(all_labels, all_predictions)\n","\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        all_labels, all_predictions, average=None, zero_division=0\n","    )\n","\n","    avg_precision = np.mean(precision)\n","    avg_recall = np.mean(recall)\n","    avg_f1 = np.mean(f1)\n","\n","    cm = confusion_matrix(all_labels, all_predictions)\n","\n","    print(f\"\\nğŸ“Š ì „ì²´ ì„±ëŠ¥ ìš”ì•½\")\n","    print(f\"{'='*30}\")\n","    print(f\"ì „ì²´ ì •í™•ë„: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n","    print(f\"í‰ê·  Precision: {avg_precision:.4f} ({avg_precision*100:.2f}%)\")\n","    print(f\"í‰ê·  Recall: {avg_recall:.4f} ({avg_recall*100:.2f}%)\")\n","    print(f\"í‰ê·  F1-Score: {avg_f1:.4f} ({avg_f1*100:.2f}%)\")\n","    print(f\"ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(all_labels)}ê°œ\")\n","\n","    return {\n","        'overall_accuracy': overall_accuracy,\n","        'class_accuracies': [class_correct[i] / max(class_total[i], 1) for i in range(len(existing_labels))],\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1,\n","        'support': support,\n","        'confusion_matrix': cm,\n","        'class_correct': class_correct,\n","        'class_total': class_total,\n","        'predictions': all_predictions,\n","        'true_labels': all_labels\n","    }\n","\n","def load_best_model_and_evaluate(model_dir, test_data_dir):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    best_model_path = os.path.join(model_dir, \"best\")\n","\n","    try:\n","        model = CLIPModel.from_pretrained(best_model_path)\n","        processor = CLIPProcessor.from_pretrained(best_model_path)\n","        model.to(device)\n","        print(\"âœ… Best ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n","    except Exception as e:\n","        print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n","        return None\n","\n","    try:\n","        with open(os.path.join(best_model_path, 'label_info.json'), 'r', encoding='utf-8') as f:\n","            label_info = json.load(f)\n","        existing_labels = label_info['labels']\n","        existing_texts = label_info['texts']\n","        print(f\"âœ… ë¼ë²¨ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(existing_labels)}ê°œ í´ë˜ìŠ¤\")\n","    except Exception as e:\n","        print(f\"âŒ ë¼ë²¨ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n","        return None\n","\n","    train_data, test_data, _, _ = load_data(test_data_dir)\n","\n","    if len(test_data) == 0:\n","        print(\"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n","        return None\n","\n","    results = detailed_evaluate_model(\n","        model, processor, test_data, existing_labels, existing_texts, device\n","    )\n","\n","    return results\n","\n","# ì‚¬ìš© ì˜ˆì‹œ\n","if __name__ == \"__main__\":\n","    # best ëª¨ë¸ë¡œ ìƒì„¸ í‰ê°€ ìˆ˜í–‰\n","    model_dir = \"/content/drive/MyDrive/trash_ai/clip_medication_test\"\n","    test_data_dir = \"/content/drive/MyDrive/trash_ai/Medicine_data\"\n","\n","    results = load_best_model_and_evaluate(model_dir, test_data_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSUixBprBsMT","executionInfo":{"status":"ok","timestamp":1748626917835,"user_tz":-540,"elapsed":5329,"user":{"displayName":"íˆí›","userId":"14574132926497786353"}},"outputId":"06a2907e-ea4b-493f-f7ac-06a6be126e27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Best ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n","âœ… ë¼ë²¨ ì •ë³´ ë¡œë“œ ì™„ë£Œ: 5ê°œ í´ë˜ìŠ¤\n","ì´ 376ê°œ ë°ì´í„° ë¡œë“œë¨\n","ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤: 5ê°œ\n","ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©: 99ê°œ\n","ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤: 162ê°œ\n","ì•ˆì•½: 37ê°œ\n","ì—°ê³ : 59ê°œ\n","ìœ ë¦¬ë³‘: 19ê°œ\n","í•™ìŠµ ë°ì´í„°: 300ê°œ\n","í…ŒìŠ¤íŠ¸ ë°ì´í„°: 76ê°œ\n","ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n","==================================================\n","ë°ì´í„°ì…‹ í¬ê¸°: 76\n","\n","ğŸ“Š ì „ì²´ ì„±ëŠ¥ ìš”ì•½\n","==============================\n","ì „ì²´ ì •í™•ë„: 0.9342 (93.42%)\n","í‰ê·  Precision: 0.9239 (92.39%)\n","í‰ê·  Recall: 0.8339 (83.39%)\n","í‰ê·  F1-Score: 0.8286 (82.86%)\n","ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: 76ê°œ\n"]}]},{"cell_type":"markdown","source":["###OCR ì¸ì‹ ë° ê²°ê³¼ ë³´ì •"],"metadata":{"id":"jowe-TMbC7uQ"}},{"cell_type":"code","source":["folder_to_label = {\n","    \"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©\": 0,\n","    \"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤\": 1,\n","    \"ì•ˆì•½\": 2,\n","    \"ì—°ê³ \": 3,\n","    \"ìœ ë¦¬ë³‘\": 4,\n","}\n","\n","medication_keywords = {\n","    \"pill_related\": [\n","        \"ìº¡ìŠ\", \"mg\", \"ì •\", \"ìº¡\", \"ì•Œì•½\", \"í™˜\", \"tablet\", \"capsule\", \"cap\", \"pill\",\n","        \"Tablet\", \"Capsule\", \"Cap\", \"Pill\", \"TABLET\", \"CAPSULE\", \"CAP\", \"PILL\",\n","        \"MG\", \"mg\", \"Mg\", \"mcg\", \"Âµg\", \"IU\", \"unit\", \"Units\", \"UNITS\"\n","    ],\n","    \"ointment_related\": [\n","        \"ì—°ê³ \", \"í¬ë¦¼\", \"ì ¤\", \"ë¡œì…˜\", \"ë°¤\", \"ì—°ê³ ì œ\", \"%\",\n","        \"ointment\", \"cream\", \"gel\", \"lotion\", \"balm\",\n","        \"Ointment\", \"Cream\", \"Gel\", \"Lotion\", \"Balm\",\n","        \"OINTMENT\", \"CREAM\", \"GEL\", \"LOTION\", \"BALM\"\n","    ],\n","    \"eye_drop_related\": [\n","        \"ì•ˆì•½\", \"ì ì•ˆì•¡\", \"ì ì•ˆì œ\", \"eye\", \"drop\", \"drops\",\n","        \"Eye\", \"Drop\", \"Drops\", \"EYE\", \"DROP\", \"DROPS\",\n","        \"ophthalmic\", \"Ophthalmic\", \"OPHTHALMIC\"\n","    ]\n","}\n","\n","def load_model(model_path=\"/content/drive/MyDrive/trash_ai/clip_medication_test/best\"):\n","    \"\"\"ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\"\"\"\n","    print(\"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n","\n","    try:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model = CLIPModel.from_pretrained(model_path)\n","        processor = CLIPProcessor.from_pretrained(model_path)\n","        model.to(device)\n","\n","        with open(os.path.join(model_path, 'label_info.json'), 'r', encoding='utf-8') as f:\n","            label_info = json.load(f)\n","\n","        print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n","        print(f\"ğŸ“‹ ë¡œë“œëœ í´ë˜ìŠ¤: {label_info['labels']}\")\n","        return model, processor, label_info\n","\n","    except Exception as e:\n","        print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n","        return None, None, None\n","\n","#OCR í…ìŠ¤íŠ¸ì—ì„œ ì˜ì•½í’ˆ ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„\n","def analyze_medication_keywords(ocr_text):\n","    if not ocr_text:\n","        return {\"pill\": 0, \"ointment\": 0, \"eye_drop\": 0}, {}\n","\n","    scores = {\n","        \"pill\": 0,\n","        \"ointment\": 0,\n","        \"eye_drop\": 0\n","    }\n","\n","    found_keywords = {\n","        \"pill\": [],\n","        \"ointment\": [],\n","        \"eye_drop\": []\n","    }\n","\n","    for keyword in medication_keywords[\"pill_related\"]:\n","        if keyword in ocr_text:\n","            scores[\"pill\"] += 1\n","            found_keywords[\"pill\"].append(keyword)\n","\n","    for keyword in medication_keywords[\"ointment_related\"]:\n","        if keyword in ocr_text:\n","            scores[\"ointment\"] += 1\n","            found_keywords[\"ointment\"].append(keyword)\n","\n","    for keyword in medication_keywords[\"eye_drop_related\"]:\n","        if keyword in ocr_text:\n","            scores[\"eye_drop\"] += 1\n","            found_keywords[\"eye_drop\"].append(keyword)\n","    return scores, found_keywords\n","\n","#í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ CLIP ì˜ˆì¸¡ ì¡°ì •\n","def adjust_clip_predictions(logits, keyword_scores, boost_factor=0.15):\n","\n","    adjusted_logits = logits.clone()\n","\n","    label_keyword_mapping = {\n","        0: \"pill\",      # ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©\n","        1: \"pill\",      # ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤\n","        2: \"eye_drop\",  # ì•ˆì•½\n","        3: \"ointment\",  # ì—°ê³ \n","        4: \"pill\",\n","    }\n","\n","    adjustments = {}\n","    for label_idx, keyword_type in label_keyword_mapping.items():\n","        if keyword_scores[keyword_type] > 0:\n","            # í‚¤ì›Œë“œ ì ìˆ˜ì— ë¹„ë¡€í•´ì„œ logit ê°’ ì¦ê°€\n","            boost = boost_factor * keyword_scores[keyword_type]\n","            adjusted_logits[0][label_idx] += boost\n","            adjustments[labels[label_idx]] = boost\n","            print(f\"ë¼ë²¨ {labels[label_idx]} ì ìˆ˜ ì¦ê°€: +{boost:.3f} (í‚¤ì›Œë“œ: {keyword_type}, ê°œìˆ˜: {keyword_scores[keyword_type]})\")\n","\n","    return adjusted_logits, adjustments\n","\n","#OCRì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ê°œì„ \n","def preprocess_image_for_ocr(image):\n","    try:\n","        img_array = np.array(image)\n","\n","        if len(img_array.shape) == 3:\n","            img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n","        else:\n","            img_bgr = img_array\n","\n","        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n","        denoised = cv2.medianBlur(gray, 3)\n","        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","        enhanced = clahe.apply(denoised)\n","        blurred = cv2.GaussianBlur(enhanced, (1, 1), 0)\n","        binary_inv = cv2.adaptiveThreshold(\n","            blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2\n","        )\n","        kernel = np.ones((1, 1), np.uint8)\n","        processed = cv2.morphologyEx(binary_inv, cv2.MORPH_CLOSE, kernel)\n","\n","        result_image = Image.fromarray(processed)\n","\n","        return result_image\n","\n","    except Exception as e:\n","        print(f\"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n","        return image\n","\n","def extract_text_from_image_with_clova(image_path: str):\n","    secret_key = userdata.get(\"OCR_CLOVA\")\n","    api_url = \"https://s79a9gvq9a.apigw.ntruss.com/custom/v1/42492/9274a7c96357207aa7e436c070ceba3fecf0b7f91679a2a8f72283f165493853/general\"\n","\n","    file_size = os.path.getsize(image_path)\n","    if file_size > 5 * 1024 * 1024:  # 5MB\n","        print(\"âš ï¸ íŒŒì¼ í¬ê¸°ê°€ 5MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.\")\n","\n","    with open(image_path, \"rb\") as f:\n","        image_data = base64.b64encode(f.read()).decode()\n","\n","    headers = {\n","        \"X-OCR-SECRET\": secret_key,\n","        \"Content-Type\": \"application/json\"\n","    }\n","\n","    payload = {\n","        \"version\": \"V2\",\n","        \"requestId\": str(uuid.uuid4()),\n","        \"timestamp\": int(time.time() * 1000),\n","        \"images\": [{\n","            \"name\": \"temp\",\n","            \"format\": \"jpg\",\n","            \"data\": image_data\n","        }]\n","    }\n","\n","    try:\n","        response = requests.post(api_url, headers=headers, data=json.dumps(payload), timeout=30)\n","        result = response.json()\n","\n","        first_image = result[\"images\"][0]\n","        fields = first_image[\"fields\"]\n","        texts = []\n","\n","        for i, field in enumerate(fields):\n","            if \"inferText\" in field and field[\"inferText\"]:\n","                text = field[\"inferText\"]\n","                texts.append(text)\n","                confidence = field.get(\"inferConfidence\", \"N/A\")\n","\n","        if not texts:\n","            return \"\", 0.0\n","\n","        extracted_text = \" \".join(texts)\n","        return extracted_text, confidence\n","\n","    except Exception as e:\n","        print(f\"âŒ OCR API í˜¸ì¶œ ì˜¤ë¥˜: {e}\")\n","        return \"\", 0.0\n","\n","def classify_and_dispose(image_path: str, model_clip, processor_clip):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    image = Image.open(image_path).convert(\"RGB\")\n","\n","    processed_image = preprocess_image_for_ocr(image)\n","    temp_path = \"temp_processed.png\"\n","    processed_image.save(temp_path)\n","\n","    # OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n","    ocr_text, ocr_confidence = extract_text_from_image_with_clova(temp_path)\n","    if ocr_text is None:\n","        ocr_text = \"\"\n","\n","    # í‚¤ì›Œë“œ ë¶„ì„\n","    keyword_scores, found_keywords = analyze_medication_keywords(ocr_text)\n","\n","    # CLIP ëª¨ë¸ë¡œ ë¶„ë¥˜\n","    inputs = processor_clip(text=text_descriptions, images=image, return_tensors=\"pt\", padding=True, truncation=True)\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        outputs = model_clip(**inputs)\n","\n","    original_logits = outputs.logits_per_image\n","    adjusted_logits, logit_adjustments = adjust_clip_predictions(original_logits, keyword_scores)\n","\n","    original_probs = original_logits.softmax(dim=1)\n","    adjusted_probs = adjusted_logits.softmax(dim=1)\n","\n","    original_predicted_idx = torch.argmax(original_probs, dim=1).item()\n","    original_predicted_label = labels[original_predicted_idx]\n","    original_confidence = torch.max(original_probs).item()\n","\n","    # ì¡°ì •ëœ ì˜ˆì¸¡\n","    predicted_idx = torch.argmax(adjusted_probs, dim=1).item()\n","    predicted_label = labels[predicted_idx]\n","    confidence = torch.max(adjusted_probs).item()\n","\n","    # OCR ê¸°ë°˜ ì¶”ê°€ ë³´ì •\n","    final_label = predicted_label\n","    ocr_correction_applied = False\n","\n","    if keyword_scores[\"ointment\"] >= 1 and predicted_label != \"ì—°ê³ \":\n","        if confidence < 0.7:\n","            final_label = \"ì—°ê³ \"\n","            confidence = min(confidence + 0.15, 0.95)\n","            ocr_correction_applied = True\n","\n","    if keyword_scores[\"eye_drop\"] >= 1 and predicted_label != \"ì•ˆì•½\":\n","        if confidence < 0.7:\n","            final_label = \"ì•ˆì•½\"\n","            confidence = min(confidence + 0.15, 0.95)\n","            ocr_correction_applied = True\n","\n","    context = disposal_map.get(final_label, \"ì¢…ë¥˜ì— ë”°ë¼ ì ì ˆí•œ ë°©ë²•ìœ¼ë¡œ ì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ì„¸ìš”.\")\n","\n","    return {\n","        \"final_label\": final_label,\n","        \"disposal_context\": context,\n","        \"final_confidence\": confidence,\n","        \"ocr_text\": ocr_text,\n","        \"ocr_confidence\" : ocr_confidence,\n","        \"original_clip_label\": original_predicted_label,\n","        \"original_clip_confidence\": original_confidence,\n","        \"adjusted_clip_label\": predicted_label,\n","        \"adjusted_clip_confidence\": torch.max(adjusted_probs).item(),\n","        \"keyword_scores\": keyword_scores,\n","        \"found_keywords\": found_keywords,\n","        \"logit_adjustments\": logit_adjustments,\n","        \"ocr_correction_applied\": ocr_correction_applied\n","    }\n","\n","def process_test_dataset(test_data_dir, model, processor, label_info):\n","    \"\"\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì „ì²´ ì²˜ë¦¬\"\"\"\n","    print(f\"\\nğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹œì‘: {test_data_dir}\")\n","    print(\"=\"*60)\n","    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n","\n","    total_processed = 0\n","    total_correct = 0\n","    class_results = defaultdict(lambda: {'correct': 0, 'total': 0, 'predictions': []})\n","\n","    # ê° í´ë˜ìŠ¤ í´ë” ì²˜ë¦¬\n","    for class_name in disposal_map.keys():\n","        class_dir = os.path.join(test_data_dir, class_name)\n","\n","        if not os.path.exists(class_dir):\n","            print(f\"âš ï¸  í´ë˜ìŠ¤ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {class_name}\")\n","            continue\n","\n","        print(f\"\\nğŸ“‚ ì²˜ë¦¬ ì¤‘: {class_name}\")\n","        image_files = []\n","        for ext in image_extensions:\n","            image_files.extend(glob.glob(os.path.join(class_dir, ext)))\n","\n","        if not image_files:\n","            print(f\"   â„¹ï¸  ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n","            continue\n","\n","        print(f\"   ğŸ“Š ë°œê²¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ\")\n","\n","        # ê° ì´ë¯¸ì§€ ì²˜ë¦¬\n","        for i, image_path in enumerate(image_files, 1):\n","            try:\n","                result = classify_and_dispose(image_path, model, processor)\n","\n","                if result:\n","                    predicted_label = result['final_label']\n","                    confidence = result['final_confidence']\n","\n","                    # ê²°ê³¼ ê¸°ë¡\n","                    class_results[class_name]['total'] += 1\n","                    class_results[class_name]['predictions'].append({\n","                        'file': os.path.basename(image_path),\n","                        'predicted': predicted_label,\n","                        'confidence': confidence,\n","                        'correct': predicted_label == class_name\n","                    })\n","\n","                    total_processed += 1\n","\n","                    if predicted_label == class_name:\n","                        class_results[class_name]['correct'] += 1\n","                        total_correct += 1\n","                        print(f\"      âœ… ì •ë‹µ: {predicted_label} (ì‹ ë¢°ë„: {confidence:.1%})\")\n","                    else:\n","                        print(f\"      âŒ ì˜¤ë‹µ: {predicted_label} (ì‹ ë¢°ë„: {confidence:.1%}) - ì •ë‹µ: {class_name}\")\n","                else:\n","                    print(f\"      âš ï¸  ì²˜ë¦¬ ì‹¤íŒ¨\")\n","\n","            except Exception as e:\n","                print(f\"      âŒ ì˜¤ë¥˜: {e}\")\n","                continue\n","\n","    return class_results, total_processed, total_correct\n","\n","def print_detailed_results(class_results, total_processed, total_correct):\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼\")\n","    print(\"=\"*60)\n","\n","    overall_accuracy = (total_correct / total_processed * 100) if total_processed > 0 else 0\n","    print(f\"\\nğŸ¯ ì „ì²´ ì„±ëŠ¥\")\n","    print(f\"   â€¢ ì „ì²´ ì •í™•ë„: {total_correct}/{total_processed} = {overall_accuracy:.2f}%\")\n","    print(f\"   â€¢ ì²˜ë¦¬ëœ ì´ ì´ë¯¸ì§€: {total_processed}ê°œ\")\n","\n","    print(f\"\\nğŸ“‹ í´ë˜ìŠ¤ë³„ ìƒì„¸ ê²°ê³¼\")\n","    print(\"-\" * 60)\n","    print(f\"{'í´ë˜ìŠ¤ëª…':<20} {'ì •í™•ë„':<15} {'ì •ë‹µ/ì „ì²´':<12} {'ì„±ëŠ¥'}\")\n","    print(\"-\" * 60)\n","\n","    for class_name, results in class_results.items():\n","        if results['total'] > 0:\n","            accuracy = results['correct'] / results['total'] * 100\n","            ratio = f\"{results['correct']}/{results['total']}\"\n","\n","            if accuracy >= 90:\n","                performance = \"ğŸŸ¢ ìš°ìˆ˜\"\n","            elif accuracy >= 70:\n","                performance = \"ğŸŸ¡ ë³´í†µ\"\n","            else:\n","                performance = \"ğŸ”´ ê°œì„ í•„ìš”\"\n","\n","            print(f\"{class_name:<20} {accuracy:<15.2f} {ratio:<12} {performance}\")\n","\n","    print(f\"\\nğŸ” ì˜¤ë¶„ë¥˜ ìƒì„¸ ë¶„ì„\")\n","    print(\"-\" * 60)\n","\n","    for class_name, results in class_results.items():\n","        wrong_predictions = [p for p in results['predictions'] if not p['correct']]\n","\n","        if wrong_predictions:\n","            print(f\"\\nâŒ {class_name} ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤:\")\n","            for pred in wrong_predictions:\n","                print(f\"   â€¢ {pred['file']} â†’ {pred['predicted']} (ì‹ ë¢°ë„: {pred['confidence']:.1%})\")\n","\n","    # ì„±ëŠ¥ ê°œì„  ì œì•ˆ\n","    print(f\"\\nğŸ’¡ ì„±ëŠ¥ ê°œì„  ì œì•ˆ\")\n","    print(\"-\" * 60)\n","\n","    low_performance_classes = []\n","    for class_name, results in class_results.items():\n","        if results['total'] > 0:\n","            accuracy = results['correct'] / results['total'] * 100\n","            if accuracy < 70:\n","                low_performance_classes.append((class_name, accuracy, results['total']))\n","\n","    if low_performance_classes:\n","        print(\"ğŸ“‰ ì„±ëŠ¥ì´ ë‚®ì€ í´ë˜ìŠ¤:\")\n","        for class_name, accuracy, total in low_performance_classes:\n","            print(f\"   â€¢ {class_name}: {accuracy:.1f}% (ìƒ˜í”Œ ìˆ˜: {total}ê°œ)\")\n","\n","    else:\n","        print(\"âœ… ëª¨ë“  í´ë˜ìŠ¤ê°€ ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!\")\n","\n","def main():\n","    print(\"ğŸ¥ ì˜ì•½í’ˆ ë¶„ë¥˜ ë° ë¶„ë¦¬ë°°ì¶œ ë„ìš°ë¯¸ (ì½˜ì†” ë²„ì „)\")\n","    print(\"=\"*60)\n","\n","    model, processor, label_info = load_model()\n","\n","    if model is None:\n","        print(\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n","        return\n","\n","    test_data_dir = \"/content/drive/MyDrive/trash_ai/final_test\"\n","    class_results, total_processed, total_correct = process_test_dataset(\n","        test_data_dir, model, processor, label_info\n","    )\n","\n","    print_detailed_results(class_results, total_processed, total_correct)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6lWC852ArZw","executionInfo":{"status":"ok","timestamp":1748626640705,"user_tz":-540,"elapsed":89572,"user":{"displayName":"íˆí›","userId":"14574132926497786353"}},"outputId":"408bb3cb-f70b-420e-e5f7-c22e66c16896"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ¥ ì˜ì•½í’ˆ ë¶„ë¥˜ ë° ë¶„ë¦¬ë°°ì¶œ ë„ìš°ë¯¸ (ì½˜ì†” ë²„ì „)\n","============================================================\n","ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...\n","âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n","ğŸ“‹ ë¡œë“œëœ í´ë˜ìŠ¤: ['ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©', 'ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤', 'ì•ˆì•½', 'ì—°ê³ ', 'ìœ ë¦¬ë³‘']\n","\n","ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹œì‘: /content/drive/MyDrive/trash_ai/final_test\n","============================================================\n","\n","ğŸ“‚ ì²˜ë¦¬ ì¤‘: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©\n","   ğŸ“Š ë°œê²¬ëœ ì´ë¯¸ì§€: 6ê°œ\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ© (ì‹ ë¢°ë„: 91.8%)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ© (ì‹ ë¢°ë„: 53.7%)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ© (ì‹ ë¢°ë„: 98.2%)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ© (ì‹ ë¢°ë„: 81.5%)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ© (ì‹ ë¢°ë„: 95.5%)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ© (ì‹ ë¢°ë„: 94.8%)\n","\n","ğŸ“‚ ì²˜ë¦¬ ì¤‘: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤\n","   ğŸ“Š ë°œê²¬ëœ ì´ë¯¸ì§€: 7ê°œ\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 99.4%)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 97.8%)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 99.3%)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 84.1%)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 99.1%)\n","ë¼ë²¨ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ© ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 1)\n","ë¼ë²¨ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 1)\n","ë¼ë²¨ ìœ ë¦¬ë³‘ ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 1)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 99.5%)\n","ë¼ë²¨ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ© ì ìˆ˜ ì¦ê°€: +0.300 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 2)\n","ë¼ë²¨ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ ì ìˆ˜ ì¦ê°€: +0.300 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 2)\n","ë¼ë²¨ ì—°ê³  ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: ointment, ê°œìˆ˜: 1)\n","ë¼ë²¨ ìœ ë¦¬ë³‘ ì ìˆ˜ ì¦ê°€: +0.300 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 2)\n","      âœ… ì •ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 99.4%)\n","\n","ğŸ“‚ ì²˜ë¦¬ ì¤‘: ì•ˆì•½\n","   ğŸ“Š ë°œê²¬ëœ ì´ë¯¸ì§€: 8ê°œ\n","      âŒ ì˜¤ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 83.0%) - ì •ë‹µ: ì•ˆì•½\n","      âœ… ì •ë‹µ: ì•ˆì•½ (ì‹ ë¢°ë„: 99.2%)\n","      âœ… ì •ë‹µ: ì•ˆì•½ (ì‹ ë¢°ë„: 98.0%)\n","      âœ… ì •ë‹µ: ì•ˆì•½ (ì‹ ë¢°ë„: 99.3%)\n","ë¼ë²¨ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ© ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 1)\n","ë¼ë²¨ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 1)\n","ë¼ë²¨ ìœ ë¦¬ë³‘ ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 1)\n","      âœ… ì •ë‹µ: ì•ˆì•½ (ì‹ ë¢°ë„: 88.8%)\n","ë¼ë²¨ ì•ˆì•½ ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: eye_drop, ê°œìˆ˜: 1)\n","      âœ… ì •ë‹µ: ì•ˆì•½ (ì‹ ë¢°ë„: 64.1%)\n","      âœ… ì •ë‹µ: ì•ˆì•½ (ì‹ ë¢°ë„: 73.0%)\n","      âœ… ì •ë‹µ: ì•ˆì•½ (ì‹ ë¢°ë„: 75.6%)\n","\n","ğŸ“‚ ì²˜ë¦¬ ì¤‘: ì—°ê³ \n","   ğŸ“Š ë°œê²¬ëœ ì´ë¯¸ì§€: 7ê°œ\n","ë¼ë²¨ ì—°ê³  ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: ointment, ê°œìˆ˜: 1)\n","      âœ… ì •ë‹µ: ì—°ê³  (ì‹ ë¢°ë„: 99.4%)\n","      âœ… ì •ë‹µ: ì—°ê³  (ì‹ ë¢°ë„: 99.9%)\n","ë¼ë²¨ ì—°ê³  ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: ointment, ê°œìˆ˜: 1)\n","      âœ… ì •ë‹µ: ì—°ê³  (ì‹ ë¢°ë„: 99.6%)\n","ë¼ë²¨ ì—°ê³  ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: ointment, ê°œìˆ˜: 1)\n","      âœ… ì •ë‹µ: ì—°ê³  (ì‹ ë¢°ë„: 97.1%)\n","      âœ… ì •ë‹µ: ì—°ê³  (ì‹ ë¢°ë„: 97.7%)\n","ë¼ë²¨ ì—°ê³  ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: ointment, ê°œìˆ˜: 1)\n","      âŒ ì˜¤ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 87.5%) - ì •ë‹µ: ì—°ê³ \n","ë¼ë²¨ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ© ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 1)\n","ë¼ë²¨ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 1)\n","ë¼ë²¨ ìœ ë¦¬ë³‘ ì ìˆ˜ ì¦ê°€: +0.150 (í‚¤ì›Œë“œ: pill, ê°œìˆ˜: 1)\n","      âœ… ì •ë‹µ: ì—°ê³  (ì‹ ë¢°ë„: 99.7%)\n","\n","ğŸ“‚ ì²˜ë¦¬ ì¤‘: ìœ ë¦¬ë³‘\n","   ğŸ“Š ë°œê²¬ëœ ì´ë¯¸ì§€: 5ê°œ\n","      âœ… ì •ë‹µ: ìœ ë¦¬ë³‘ (ì‹ ë¢°ë„: 77.5%)\n","      âŒ ì˜¤ë‹µ: ì•ˆì•½ (ì‹ ë¢°ë„: 74.8%) - ì •ë‹µ: ìœ ë¦¬ë³‘\n","      âŒ ì˜¤ë‹µ: ì•ˆì•½ (ì‹ ë¢°ë„: 77.9%) - ì •ë‹µ: ìœ ë¦¬ë³‘\n","      âŒ ì˜¤ë‹µ: ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 91.9%) - ì •ë‹µ: ìœ ë¦¬ë³‘\n","      âœ… ì •ë‹µ: ìœ ë¦¬ë³‘ (ì‹ ë¢°ë„: 54.0%)\n","\n","============================================================\n","ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼\n","============================================================\n","\n","ğŸ¯ ì „ì²´ ì„±ëŠ¥\n","   â€¢ ì „ì²´ ì •í™•ë„: 28/33 = 84.85%\n","   â€¢ ì²˜ë¦¬ëœ ì´ ì´ë¯¸ì§€: 33ê°œ\n","\n","ğŸ“‹ í´ë˜ìŠ¤ë³„ ìƒì„¸ ê²°ê³¼\n","------------------------------------------------------------\n","í´ë˜ìŠ¤ëª…                 ì •í™•ë„             ì •ë‹µ/ì „ì²´        ì„±ëŠ¥\n","------------------------------------------------------------\n","ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©             100.00          6/6          ğŸŸ¢ ìš°ìˆ˜\n","ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤          100.00          7/7          ğŸŸ¢ ìš°ìˆ˜\n","ì•ˆì•½                   87.50           7/8          ğŸŸ¡ ë³´í†µ\n","ì—°ê³                    85.71           6/7          ğŸŸ¡ ë³´í†µ\n","ìœ ë¦¬ë³‘                  40.00           2/5          ğŸ”´ ê°œì„ í•„ìš”\n","\n","ğŸ” ì˜¤ë¶„ë¥˜ ìƒì„¸ ë¶„ì„\n","------------------------------------------------------------\n","\n","âŒ ì•ˆì•½ ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤:\n","   â€¢ 2.jpg â†’ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 83.0%)\n","\n","âŒ ì—°ê³  ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤:\n","   â€¢ image.png â†’ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 87.5%)\n","\n","âŒ ìœ ë¦¬ë³‘ ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤:\n","   â€¢ 20230922115540_80A32.jpg â†’ ì•ˆì•½ (ì‹ ë¢°ë„: 74.8%)\n","   â€¢ output_1261293561.jpg â†’ ì•ˆì•½ (ì‹ ë¢°ë„: 77.9%)\n","   â€¢ 72432_60742_3559.jpg â†’ ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤ (ì‹ ë¢°ë„: 91.9%)\n","\n","ğŸ’¡ ì„±ëŠ¥ ê°œì„  ì œì•ˆ\n","------------------------------------------------------------\n","ğŸ“‰ ì„±ëŠ¥ì´ ë‚®ì€ í´ë˜ìŠ¤:\n","   â€¢ ìœ ë¦¬ë³‘: 40.0% (ìƒ˜í”Œ ìˆ˜: 5ê°œ)\n"]}]},{"cell_type":"markdown","source":["### streamlit_code"],"metadata":{"id":"Z2Dcr0LADOOF"}},{"cell_type":"code","source":["streamlit_code = '''\n","import json\n","import streamlit as st\n","import torch\n","from PIL import Image\n","from transformers import CLIPProcessor, CLIPModel\n","import os\n","from google.colab import userdata\n","import cv2\n","import numpy as np\n","import requests\n","import base64\n","import uuid\n","import time\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","st.set_page_config(\n","    page_title=\"ì˜ì•½í’ˆ ë¶„ë¦¬ë°°ì¶œ AI\",\n","    page_icon=\"ğŸ¥\",\n","    layout=\"wide\"\n",")\n","\n","disposal_map = {\n","    \"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©\": \"ê²‰ í¬ì¥ì§€(ì¢…ì´ ë˜ëŠ” í”Œë¼ìŠ¤í‹± í•„ë¦„)ë¥¼ ì œê±°í•˜ê³ , í”Œë¼ìŠ¤í‹± ë¸”ë¦¬ìŠ¤í„°íŒ©ì€ ê·¸ëŒ€ë¡œ ì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ì„¸ìš”.\",\n","    \"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤\": \"ì¢…ì´ ë°•ìŠ¤ëŠ” ë¶„ë¦¬í•˜ì—¬ ì¢…ì´ ì¬í™œìš©í•¨ì— ë²„ë¦¬ê³ , ê²‰ í¬ì¥ì§€(ì¢…ì´ ë˜ëŠ” í”Œë¼ìŠ¤í‹± í•„ë¦„)ë¥¼ ì œê±°í•œ í›„ í”Œë¼ìŠ¤í‹± ë¸”ë¦¬ìŠ¤í„°íŒ©ì€ ì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ì„¸ìš”.\",\n","    \"ìœ ë¦¬ë³‘\": \"ì•½ì€ íì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ê³ , ìœ ë¦¬ë³‘ì€ ë‚´ìš©ë¬¼ì„ ì™„ì „íˆ ë¹„ìš´ í›„ ê¹¨ë—ì´ í—¹ê¶ˆ ë³‘ë¥˜ë¡œ ë¶„ë¦¬ë°°ì¶œí•˜ì„¸ìš”.\",\n","    \"ì•ˆì•½\": \"ìš©ê¸° ê·¸ëŒ€ë¡œ ì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ì„¸ìš”. ë‚¨ì€ ì•½ì•¡ì€ ê·¸ëŒ€ë¡œ ë‘ì…”ë„ ë©ë‹ˆë‹¤.\",\n","    \"ì—°ê³ \": \"íŠœë¸Œë¥¼ ì™„ì „íˆ ë¹„ìš°ê³  ì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ì„¸ìš”.\"\n","}\n","\n","text_descriptions = [\n","    \"pill blister pack with plastic packaging\",\n","    \"pill blister pack in cardboard box\",\n","    \"glass bottle for medicine\",\n","    \"small eye drop bottle\",\n","    \"ointment tube for skin\"\n","]\n","\n","labels = [\"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©\", \"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤\", \"ìœ ë¦¬ë³‘\", \"ì•ˆì•½\", \"ì—°ê³ \"]\n","\n","folder_to_label = {\n","    \"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©\": 0,\n","    \"ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤\": 1,\n","    \"ìœ ë¦¬ë³‘\": 2,\n","    \"ì•ˆì•½\": 3,\n","    \"ì—°ê³ \": 4\n","}\n","\n","medication_keywords = {\n","    \"pill_related\": [\n","        \"ìº¡ìŠ\", \"mg\", \"ì •\", \"ìº¡\", \"ì•Œì•½\", \"í™˜\", \"tablet\", \"capsule\", \"cap\", \"pill\",\n","        \"Tablet\", \"Capsule\", \"Cap\", \"Pill\", \"TABLET\", \"CAPSULE\", \"CAP\", \"PILL\",\n","        \"MG\", \"mg\", \"Mg\", \"mcg\", \"Âµg\", \"IU\", \"unit\", \"Units\", \"UNITS\"\n","    ],\n","    \"ointment_related\": [\n","        \"ì—°ê³ \", \"í¬ë¦¼\", \"ì ¤\", \"ë¡œì…˜\", \"ë°¤\", \"ì—°ê³ ì œ\", \"%\",\n","        \"ointment\", \"cream\", \"gel\", \"lotion\", \"balm\",\n","        \"Ointment\", \"Cream\", \"Gel\", \"Lotion\", \"Balm\",\n","        \"OINTMENT\", \"CREAM\", \"GEL\", \"LOTION\", \"BALM\"\n","    ],\n","    \"eye_drop_related\": [\n","        \"ì•ˆì•½\", \"ì ì•ˆì•¡\", \"ì ì•ˆì œ\", \"eye\", \"drop\", \"drops\",\n","        \"Eye\", \"Drop\", \"Drops\", \"EYE\", \"DROP\", \"DROPS\",\n","        \"ophthalmic\", \"Ophthalmic\", \"OPHTHALMIC\"\n","    ]\n","}\n","\n","def analyze_medication_keywords(ocr_text):\n","    if not ocr_text:\n","        return {\"pill\": 0, \"ointment\": 0, \"eye_drop\": 0}, {}\n","\n","    scores = {\n","        \"pill\": 0,\n","        \"ointment\": 0,\n","        \"eye_drop\": 0\n","    }\n","\n","    found_keywords = {\n","        \"pill\": [],\n","        \"ointment\": [],\n","        \"eye_drop\": []\n","    }\n","\n","    for keyword in medication_keywords[\"pill_related\"]:\n","        if keyword in ocr_text:\n","            scores[\"pill\"] += 1\n","            found_keywords[\"pill\"].append(keyword)\n","\n","    for keyword in medication_keywords[\"ointment_related\"]:\n","        if keyword in ocr_text:\n","            scores[\"ointment\"] += 1\n","            found_keywords[\"ointment\"].append(keyword)\n","\n","    for keyword in medication_keywords[\"eye_drop_related\"]:\n","        if keyword in ocr_text:\n","            scores[\"eye_drop\"] += 1\n","            found_keywords[\"eye_drop\"].append(keyword)\n","    return scores, found_keywords\n","\n","def adjust_clip_predictions(logits, keyword_scores, boost_factor=0.15):\n","\n","    adjusted_logits = logits.clone()\n","\n","    label_keyword_mapping = {\n","        0: \"pill\",      # ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©\n","        1: \"pill\",      # ì•Œì•½_ë¸”ë¦¬ìŠ¤í„°íŒ©_ë°•ìŠ¤\n","        2: \"pill\",      # ìœ ë¦¬ë³‘\n","        3: \"eye_drop\",  # ì•ˆì•½\n","        4: \"ointment\"   # ì—°ê³ \n","    }\n","\n","    adjustments = {}\n","    for label_idx, keyword_type in label_keyword_mapping.items():\n","        if keyword_scores[keyword_type] > 0:\n","            boost = boost_factor * keyword_scores[keyword_type]\n","            adjusted_logits[0][label_idx] += boost\n","            adjustments[labels[label_idx]] = boost\n","            print(f\"ë¼ë²¨ {labels[label_idx]} ì ìˆ˜ ì¦ê°€: +{boost:.3f} (í‚¤ì›Œë“œ: {keyword_type}, ê°œìˆ˜: {keyword_scores[keyword_type]})\")\n","\n","    return adjusted_logits, adjustments\n","\n","def preprocess_image_for_ocr(image):\n","    try:\n","        img_array = np.array(image)\n","\n","        if len(img_array.shape) == 3:\n","            img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n","        else:\n","            img_bgr = img_array\n","\n","        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n","        denoised = cv2.medianBlur(gray, 3)\n","        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","        enhanced = clahe.apply(denoised)\n","        blurred = cv2.GaussianBlur(enhanced, (1, 1), 0)\n","        binary_inv = cv2.adaptiveThreshold(\n","            blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2\n","        )\n","        kernel = np.ones((1, 1), np.uint8)\n","        processed = cv2.morphologyEx(binary_inv, cv2.MORPH_CLOSE, kernel)\n","\n","        result_image = Image.fromarray(processed)\n","\n","        return result_image\n","\n","    except Exception as e:\n","        print(f\"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n","        return image\n","\n","def extract_text_from_image_with_clova(image_path: str):\n","    secret_key = userdata.get(\"OCR_CLOVA\")\n","    api_url = \"https://s79a9gvq9a.apigw.ntruss.com/custom/v1/42492/9274a7c96357207aa7e436c070ceba3fecf0b7f91679a2a8f72283f165493853/general\"\n","\n","    file_size = os.path.getsize(image_path)\n","    if file_size > 5 * 1024 * 1024:  # 5MB\n","        print(\"âš ï¸ íŒŒì¼ í¬ê¸°ê°€ 5MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.\")\n","\n","    with open(image_path, \"rb\") as f:\n","        image_data = base64.b64encode(f.read()).decode()\n","\n","    headers = {\n","        \"X-OCR-SECRET\": secret_key,\n","        \"Content-Type\": \"application/json\"\n","    }\n","\n","    payload = {\n","        \"version\": \"V2\",\n","        \"requestId\": str(uuid.uuid4()),\n","        \"timestamp\": int(time.time() * 1000),\n","        \"images\": [{\n","            \"name\": \"temp\",\n","            \"format\": \"jpg\",\n","            \"data\": image_data\n","        }]\n","    }\n","\n","    try:\n","        response = requests.post(api_url, headers=headers, data=json.dumps(payload), timeout=30)\n","        result = response.json()\n","\n","        first_image = result[\"images\"][0]\n","        fields = first_image[\"fields\"]\n","        texts = []\n","\n","        for i, field in enumerate(fields):\n","            if \"inferText\" in field and field[\"inferText\"]:\n","                text = field[\"inferText\"]\n","                texts.append(text)\n","                confidence = field.get(\"inferConfidence\", \"N/A\")\n","                print(f\"í…ìŠ¤íŠ¸ {i+1}: '{text}' (ì‹ ë¢°ë„: {confidence})\")\n","\n","        if not texts:\n","            return \"\", 0.0\n","\n","        extracted_text = \" \".join(texts)\n","        return extracted_text, confidence\n","\n","    except Exception as e:\n","        print(f\"âŒ OCR API í˜¸ì¶œ ì˜¤ë¥˜: {e}\")\n","        return \"\", 0.0\n","\n","def classify_and_dispose(image_path: str, model_clip, processor_clip):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    image = Image.open(image_path).convert(\"RGB\")\n","\n","    processed_image = preprocess_image_for_ocr(image)\n","    temp_path = \"temp_processed.png\"\n","    processed_image.save(temp_path)\n","\n","    ocr_text, ocr_confidence = extract_text_from_image_with_clova(temp_path)\n","    if ocr_text is None:\n","        ocr_text = \"\"\n","\n","    keyword_scores, found_keywords = analyze_medication_keywords(ocr_text)\n","\n","    inputs = processor_clip(text=text_descriptions, images=image, return_tensors=\"pt\", padding=True, truncation=True)\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        outputs = model_clip(**inputs)\n","\n","    original_logits = outputs.logits_per_image\n","    adjusted_logits, logit_adjustments = adjust_clip_predictions(original_logits, keyword_scores)\n","\n","    original_probs = original_logits.softmax(dim=1)\n","    adjusted_probs = adjusted_logits.softmax(dim=1)\n","\n","    original_predicted_idx = torch.argmax(original_probs, dim=1).item()\n","    original_predicted_label = labels[original_predicted_idx]\n","    original_confidence = torch.max(original_probs).item()\n","\n","    predicted_idx = torch.argmax(adjusted_probs, dim=1).item()\n","    predicted_label = labels[predicted_idx]\n","    confidence = torch.max(adjusted_probs).item()\n","\n","    final_label = predicted_label\n","    ocr_correction_applied = False\n","\n","    if keyword_scores[\"ointment\"] >= 1 and predicted_label != \"ì—°ê³ \":\n","        if confidence < 0.7:\n","            final_label = \"ì—°ê³ \"\n","            confidence = min(confidence + 0.15, 0.95)\n","            ocr_correction_applied = True\n","\n","    if keyword_scores[\"eye_drop\"] >= 1 and predicted_label != \"ì•ˆì•½\":\n","        if confidence < 0.7:\n","            final_label = \"ì•ˆì•½\"\n","            confidence = min(confidence + 0.15, 0.95)\n","            ocr_correction_applied = True\n","\n","    context = disposal_map.get(final_label, \"ì¢…ë¥˜ì— ë”°ë¼ ì ì ˆí•œ ë°©ë²•ìœ¼ë¡œ ì˜ì•½í’ˆ ìˆ˜ê±°í•¨ì— ë²„ë¦¬ì„¸ìš”.\")\n","\n","    return {\n","        \"final_label\": final_label,\n","        \"disposal_context\": context,\n","        \"final_confidence\": confidence,\n","        \"ocr_text\": ocr_text,\n","        \"ocr_confidence\" : ocr_confidence,\n","        \"original_clip_label\": original_predicted_label,\n","        \"original_clip_confidence\": original_confidence,\n","        \"adjusted_clip_label\": predicted_label,\n","        \"adjusted_clip_confidence\": torch.max(adjusted_probs).item(),\n","        \"keyword_scores\": keyword_scores,\n","        \"found_keywords\": found_keywords,\n","        \"logit_adjustments\": logit_adjustments,\n","        \"ocr_correction_applied\": ocr_correction_applied\n","    }\n","\n","@st.cache_resource\n","def load_model():\n","    try:\n","        model_path = \"/content/drive/MyDrive/trash_ai/clip_medication_test/best\"\n","        if os.path.exists(model_path):\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            model = CLIPModel.from_pretrained(model_path).to(device)\n","            processor = CLIPProcessor.from_pretrained(model_path)\n","            return model, processor\n","        else:\n","            st.error(f\"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n","            return None, None\n","    except Exception as e:\n","        st.error(f\"âŒ ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {str(e)}\")\n","        return None, None\n","\n","def main():\n","    st.title(\"ğŸ¥ ì˜ì•½í’ˆ ë¶„ë¦¬ë°°ì¶œ AI\")\n","    st.markdown(\"---\")\n","    st.write(\"ì˜ì•½í’ˆ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ ì¢…ë¥˜ë¥¼ ë¶„ë¥˜í•˜ê³  ì˜¬ë°”ë¥¸ ë¶„ë¦¬ë°°ì¶œ ë°©ë²•ì„ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤.\")\n","\n","    with st.sidebar:\n","        st.header(\"ğŸ“‹ ë¶„ë¥˜ ê°€ëŠ¥í•œ ì˜ì•½í’ˆ\")\n","        st.write(\"â€¢ ğŸ’Š ì•Œì•½ ë¸”ë¦¬ìŠ¤í„°íŒ©\")\n","        st.write(\"â€¢ ğŸ“¦ ì•Œì•½ ë¸”ë¦¬ìŠ¤í„°íŒ© (ë°•ìŠ¤ í¬í•¨)\")\n","        st.write(\"â€¢ ğŸ¼ ìœ ë¦¬ë³‘\")\n","        st.write(\"â€¢ ğŸ‘ï¸ ì•ˆì•½\")\n","        st.write(\"â€¢ ğŸ§´ ì—°ê³ \")\n","\n","        st.header(\"ğŸ“– ì‚¬ìš© ë°©ë²•\")\n","        st.write(\"1. ì˜ì•½í’ˆ ì‚¬ì§„ ì—…ë¡œë“œ\")\n","        st.write(\"2. AI ìë™ ë¶„ì„ ëŒ€ê¸°\")\n","        st.write(\"3. ë¶„ë¦¬ë°°ì¶œ ë°©ë²• í™•ì¸\")\n","        st.warning(\"âš ï¸ í…ìŠ¤íŠ¸ê°€ ì„ ëª…í•˜ê²Œ ë³´ì´ë„ë¡ ì´¬ì˜í•˜ë©´ ë” ì •í™•í•œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n","\n","        st.header(\"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´\")\n","        device = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n","        st.write(f\"ë””ë°”ì´ìŠ¤: {device}\")\n","\n","    with st.spinner(\"ëª¨ë¸ ë¡œë”© ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.\"):\n","        model_clip, processor_clip = load_model()\n","\n","    if model_clip is None or processor_clip is None:\n","        st.error(\"ğŸš« ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ê²½ë¡œì™€ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","        st.info(\"ğŸ’¡ ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n","        return\n","\n","    st.success(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n","\n","    uploaded_file = st.file_uploader(\n","        \"ğŸ“¸ ì˜ì•½í’ˆ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”\",\n","        type=[\"jpg\", \"jpeg\", \"png\"],\n","        help=\"JPG, JPEG, PNG í˜•ì‹ì˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ê°€ ì„ ëª…í•˜ê²Œ ë³´ì´ëŠ” ì´ë¯¸ì§€ì¼ìˆ˜ë¡ ì •í™•ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.\"\n","    )\n","\n","    if uploaded_file is not None:\n","        try:\n","            col1, col2 = st.columns([1, 1])\n","\n","            with col1:\n","                st.subheader(\"ğŸ“· ì—…ë¡œë“œëœ ì´ë¯¸ì§€\")\n","                st.image(uploaded_file, caption=\"ë¶„ì„í•  ì´ë¯¸ì§€\", use_container_width=True)\n","\n","            temp_path = f\"temp_{uploaded_file.name}\"\n","            with open(temp_path, \"wb\") as f:\n","                f.write(uploaded_file.getbuffer())\n","\n","            if st.button(\"ğŸ” ë¶„ì„ ì‹œì‘\", type=\"primary\"):\n","                with col2:\n","                    st.subheader(\"ğŸ“Š ë¶„ì„ ê²°ê³¼\")\n","\n","                    # í”„ë¡œê·¸ë ˆìŠ¤ ë°”\n","                    progress_bar = st.progress(0)\n","                    status_text = st.empty()\n","\n","                    status_text.text(\"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘...\")\n","                    progress_bar.progress(20)\n","\n","                    status_text.text(\"OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\")\n","                    progress_bar.progress(40)\n","\n","                    status_text.text(\"í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...\")\n","                    progress_bar.progress(50)\n","\n","                    status_text.text(\"AI ëª¨ë¸ ì‹¤í–‰ ì¤‘...\")\n","                    progress_bar.progress(70)\n","\n","                    result = classify_and_dispose(temp_path, model_clip, processor_clip)\n","\n","                    progress_bar.progress(90)\n","                    status_text.text(\"ê²°ê³¼ ìƒì„± ì¤‘...\")\n","\n","                    progress_bar.progress(100)\n","                    status_text.text(\"ë¶„ì„ ì™„ë£Œ!\")\n","\n","                    st.success(\"âœ… ë¶„ì„ ì™„ë£Œ!\")\n","\n","                    st.markdown(f\"\"\"\n","                    <div style=\"padding: 1rem; border-radius: 0.5rem; background-color: #e8f4fd; margin: 1rem 0; border-left: 4px solid #1f77b4;\">\n","                        <h3>ğŸ” ìµœì¢… ë¶„ë¥˜ ê²°ê³¼: {result['final_label']}</h3>\n","                    </div>\n","                    \"\"\", unsafe_allow_html=True)\n","\n","                    if result['final_confidence'] > 0.8:\n","                        st.success(f\"ğŸ“Š ì‹ ë¢°ë„: {result['final_confidence']:.1%} (ë†’ìŒ)\")\n","                    elif result['final_confidence'] > 0.6:\n","                        st.warning(f\"ğŸ“Š ì‹ ë¢°ë„: {result['final_confidence']:.1%} (ë³´í†µ)\")\n","                    else:\n","                        st.error(f\"ğŸ“Š ì‹ ë¢°ë„: {result['final_confidence']:.1%} (ë‚®ìŒ)\")\n","                        st.warning(\"ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.\")\n","\n","                    st.info(f\"\"\"â™»ï¸ **ë¶„ë¦¬ë°°ì¶œ ë°©ë²•**: {result['disposal_context']}\"\"\")\n","\n","                    with st.expander(\"ğŸ” ë¶„ì„ ê³¼ì • ìƒì„¸ ì •ë³´\", expanded=True):\n","                        st.markdown(\"### 1ï¸âƒ£ CLIP ëª¨ë¸ ì›ë³¸ ì˜ˆì¸¡\")\n","                        st.write(f\"- ì˜ˆì¸¡: **{result['original_clip_label']}**\")\n","                        st.write(f\"- ì‹ ë¢°ë„: {result['original_clip_confidence']:.1%}\")\n","\n","                        st.markdown(\"### 2ï¸âƒ£ OCR í‚¤ì›Œë“œ ë¶„ì„\")\n","                        if result['ocr_text'] and result['ocr_confidence']:\n","                            st.write(\"**ì¶”ì¶œëœ í…ìŠ¤íŠ¸:**\")\n","                            st.code(result['ocr_text'])\n","                            st.write(\"**OCR ê²°ê³¼ ì‹ ë¢°ë„:**\")\n","                            st.code(result['ocr_confidence'])\n","\n","                            col_k1, col_k2 = st.columns(2)\n","                            with col_k1:\n","                                st.write(\"**í‚¤ì›Œë“œ ì ìˆ˜:**\")\n","                                for category, score in result['keyword_scores'].items():\n","                                    emoji = {\"pill\": \"ğŸ’Š\", \"liquid\": \"ğŸ§´\", \"ointment\": \"ğŸ§´\", \"eye_drop\": \"ğŸ‘ï¸\"}\n","                                    st.write(f\"{emoji.get(category, 'â€¢')} {category}: {score}ê°œ\")\n","\n","                            with col_k2:\n","                                st.write(\"**ë°œê²¬ëœ í‚¤ì›Œë“œ:**\")\n","                                for category, keywords in result['found_keywords'].items():\n","                                    if keywords:\n","                                        st.write(f\"**{category}:** {', '.join(keywords)}\")\n","                        else:\n","                            st.warning(\"OCRë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n","\n","                        st.markdown(\"### 3ï¸âƒ£ í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ì¡°ì •\")\n","                        if result['logit_adjustments']:\n","                            st.write(\"**ì ìˆ˜ ì¡°ì • ë‚´ì—­:**\")\n","                            for label, boost in result['logit_adjustments'].items():\n","                                st.write(f\"- {label}: +{boost:.3f}\")\n","                            st.write(f\"- ì¡°ì • í›„ ì˜ˆì¸¡: **{result['adjusted_clip_label']}**\")\n","                            st.write(f\"- ì¡°ì • í›„ ì‹ ë¢°ë„: {result['adjusted_clip_confidence']:.1%}\")\n","                        else:\n","                            st.write(\"í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ì¡°ì •ì´ ì ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n","\n","                        st.markdown(\"### 4ï¸âƒ£ OCR ê¸°ë°˜ ìµœì¢… ë³´ì •\")\n","                        if result['ocr_correction_applied']:\n","                            st.success(f\"âœ… OCR í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ '{result['final_label']}'ë¡œ ìµœì¢… ë³´ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","                        else:\n","                            st.info(\"OCR ê¸°ë°˜ ì¶”ê°€ ë³´ì •ì´ ì ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n","\n","                        st.markdown(\"### ğŸ“‹ ë³€ê²½ ì‚¬í•­ ìš”ì•½\")\n","                        changes = []\n","                        if result['original_clip_label'] != result['adjusted_clip_label']:\n","                            changes.append(f\"CLIP í‚¤ì›Œë“œ ì¡°ì •: {result['original_clip_label']} â†’ {result['adjusted_clip_label']}\")\n","                        if result['ocr_correction_applied']:\n","                            changes.append(f\"OCR ê¸°ë°˜ ë³´ì •: {result['adjusted_clip_label']} â†’ {result['final_label']}\")\n","\n","                        if changes:\n","                            for change in changes:\n","                                st.write(f\"â€¢ {change}\")\n","                        else:\n","                            st.write(\"â€¢ ì›ë³¸ CLIP ì˜ˆì¸¡ì´ ê·¸ëŒ€ë¡œ ìœ ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","\n","            if os.path.exists(temp_path):\n","                os.remove(temp_path)\n","\n","        except Exception as e:\n","            st.error(f\"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n","            st.write(\"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´:\")\n","            st.code(str(e))\n","\n","    st.markdown(\"---\")\n","    st.markdown(\n","        \"<div style='text-align: center; color: #666;'>\"\n","        \"<p>ğŸŒ± í™˜ê²½ì„ ìƒê°í•˜ëŠ” AI ê¸°ë°˜ ì˜ì•½í’ˆ ë¶„ë¦¬ë°°ì¶œ ë„ìš°ë¯¸ (í‚¤ì›Œë“œ ë¶„ì„ ì‹œê°í™” ë²„ì „)</p>\"\n","        \"</div>\",\n","        unsafe_allow_html=True\n","    )\n","\n","if __name__ == \"__main__\":\n","    main()\n","'''\n","\n","with open(\"/content/drive/MyDrive/trash_ai/medicine_app.py\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(streamlit_code)"],"metadata":{"id":"etovEbT3tOGq","executionInfo":{"status":"ok","timestamp":1750607980262,"user_tz":-540,"elapsed":1332,"user":{"displayName":"ê¹€ì„¸í¬","userId":"10121931475775017609"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import threading\n","import subprocess\n","import time\n","from google.colab import output\n","\n","def start_streamlit():\n","    subprocess.run([\n","        \"streamlit\", \"run\", \"/content/drive/MyDrive/trash_ai/medicine_app.py\",\n","        \"--server.port\", \"8502\",\n","        \"--server.address\", \"0.0.0.0\",\n","        \"--server.headless\", \"true\",\n","        \"--server.fileWatcherType\", \"none\",\n","        \"--server.enableXsrfProtection\", \"false\",\n","        \"--browser.gatherUsageStats\", \"false\",\n","        \"--server.enableWebsocketCompression\", \"false\",\n","        \"--server.enableCORS\", \"false\"\n","    ])\n","\n","thread = threading.Thread(target=start_streamlit)\n","thread.daemon = True\n","thread.start()\n","\n","print(\"â³ ì˜ì•½í’ˆ ë¶„ë¦¬ë°°ì¶œ AI ì‹œì‘ ì¤‘...\")\n","print(\"ë¸Œë¼ìš°ì €ì—ì„œ ì•„ë˜ URLë¡œ ì ‘ì†í•˜ì„¸ìš”:\")\n","\n","# Colabì˜ ê¶Œì¥ ë°©ë²• ì‚¬ìš© - iframe\n","try:\n","    output.serve_kernel_port_as_iframe(8502, height=800)\n","except:\n","    print(\"http://localhost:8502\")\n","    print(\"ë˜ëŠ” í¬íŠ¸ í¬ì›Œë”© URLì„ ì‚¬ìš©í•˜ì„¸ìš”.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":857},"id":"oqFf64X60bjp","outputId":"4f344593-64b2-4a41-dc4a-0de4ebe8cf4d","executionInfo":{"status":"ok","timestamp":1750604967013,"user_tz":-540,"elapsed":13,"user":{"displayName":"ê¹€ì„¸í¬","userId":"10121931475775017609"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["â³ ì˜ì•½í’ˆ ë¶„ë¦¬ë°°ì¶œ AI ì‹œì‘ ì¤‘...\n","ë¸Œë¼ìš°ì €ì—ì„œ ì•„ë˜ URLë¡œ ì ‘ì†í•˜ì„¸ìš”:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","    if (!google.colab.kernel.accessAllowed && !cache) {\n","      return;\n","    }\n","    element.appendChild(document.createTextNode(''));\n","    const url = await google.colab.kernel.proxyPort(port, {cache});\n","    const iframe = document.createElement('iframe');\n","    iframe.src = new URL(path, url).toString();\n","    iframe.height = height;\n","    iframe.width = width;\n","    iframe.style.border = 0;\n","    iframe.allow = [\n","        'accelerometer',\n","        'autoplay',\n","        'camera',\n","        'clipboard-read',\n","        'clipboard-write',\n","        'gyroscope',\n","        'magnetometer',\n","        'microphone',\n","        'serial',\n","        'usb',\n","        'xr-spatial-tracking',\n","    ].join('; ');\n","    element.appendChild(iframe);\n","  })(8502, \"/\", \"100%\", 800, false, window.element)"]},"metadata":{}}]},{"cell_type":"code","source":["def kill_existing_streamlit():\n","    try:\n","        result = subprocess.run(['pgrep', '-f', 'streamlit'], capture_output=True, text=True)\n","        if result.stdout:\n","            pids = result.stdout.strip().split('\\n')\n","            for pid in pids:\n","                if pid:\n","                    try:\n","                        os.kill(int(pid), signal.SIGTERM)\n","                        print(f\"ì¢…ë£Œëœ PID: {pid}\")\n","                    except:\n","                        pass\n","        subprocess.run(['fuser', '-k', '8501/tcp'], capture_output=True)\n","        print(\"ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ\")\n","    except Exception as e:\n","        print(f\"í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n","\n","kill_existing_streamlit()"],"metadata":{"id":"Dj3nONdn1GdY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750604869381,"user_tz":-540,"elapsed":37,"user":{"displayName":"ê¹€ì„¸í¬","userId":"10121931475775017609"}},"outputId":"798de5b6-1601-42e6-b08a-477de7115fa3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ\n"]}]}]}